{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3342923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StreamPulse-PlanAudit\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0922f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# Listening events (large table - 500K rows)\n",
    "events_data = []\n",
    "for i in range(500000):\n",
    "    events_data.append((\n",
    "        f\"EVT-{i+1:07d}\",\n",
    "        f\"USR-{random.randint(1, 100000):06d}\",\n",
    "        f\"TRK-{random.randint(1, 50000):06d}\",\n",
    "        f\"ART-{random.randint(1, 5000):05d}\",\n",
    "        random.randint(10, 300),\n",
    "        random.choice([True, False]),\n",
    "        random.choice([\"mobile\", \"desktop\", \"smart_speaker\", \"tablet\"]),\n",
    "        random.choice([\"free\", \"premium\"]),\n",
    "        f\"202{random.randint(3,4)}-{random.randint(1,12):02d}-{random.randint(1,28):02d}\",\n",
    "    ))\n",
    "\n",
    "events = spark.createDataFrame(events_data,\n",
    "    [\"event_id\", \"user_id\", \"track_id\", \"artist_id\", \"duration_sec\",\n",
    "     \"completed\", \"device\", \"tier\", \"event_date\"]) \\\n",
    "    .withColumn(\"event_date\", col(\"event_date\").cast(\"date\")) \\\n",
    "    .withColumn(\"year\", year(col(\"event_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"event_date\")))\n",
    "\n",
    "events.write.parquet(\"audit_data/events\", mode=\"overwrite\", partitionBy=[\"year\"])\n",
    "\n",
    "# Artists (small table - 5K rows)\n",
    "artist_data = [(f\"ART-{i+1:05d}\", f\"Artist {i+1}\",\n",
    "                random.choice([\"Pop\", \"Rock\", \"Hip-Hop\", \"Jazz\", \"Electronic\"]),\n",
    "                random.choice([\"US\", \"UK\", \"KR\", \"JP\", \"DE\"]))\n",
    "               for i in range(5000)]\n",
    "artists = spark.createDataFrame(artist_data, [\"artist_id\", \"name\", \"genre\", \"country\"])\n",
    "artists.write.parquet(\"audit_data/artists\", mode=\"overwrite\")\n",
    "\n",
    "# Tracks (medium table - 50K rows)\n",
    "track_data = [(f\"TRK-{i+1:06d}\", f\"Track {i+1}\",\n",
    "               f\"ART-{random.randint(1, 5000):05d}\",\n",
    "               random.randint(60, 400),\n",
    "               random.randint(2018, 2024))\n",
    "              for i in range(50000)]\n",
    "tracks = spark.createDataFrame(track_data,\n",
    "    [\"track_id\", \"title\", \"artist_id\", \"track_duration\", \"release_year\"])\n",
    "tracks.write.parquet(\"audit_data/tracks\", mode=\"overwrite\")\n",
    "\n",
    "# Reload from Parquet\n",
    "events = spark.read.parquet(\"audit_data/events\")\n",
    "artists = spark.read.parquet(\"audit_data/artists\")\n",
    "tracks = spark.read.parquet(\"audit_data/tracks\")\n",
    "\n",
    "print(f\"Events: {events.count()} | Artists: {artists.count()} | Tracks: {tracks.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Simple filter and select\n",
    "q1 = events.filter(col(\"year\") == 2024) \\\n",
    "    .filter(col(\"completed\") == True) \\\n",
    "    .select(\"event_id\", \"user_id\", \"duration_sec\")\n",
    "\n",
    "print(\"QUERY 1: Simple filter and select\")\n",
    "q1.explain(mode=\"formatted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66077361",
   "metadata": {},
   "source": [
    "### Interpreting the formatted explain output\n",
    "\n",
    "Assuming a typical Spark setup (Parquet/Delta source, year as a partition column), here’s how to read it:\n",
    "\n",
    "\n",
    "| Aspect                 | Your Finding                                                                                      |\n",
    "| ---------------------- | ------------------------------------------------------------------------------------------------- |\n",
    "| **Scan type**          | `FileScan` (typically Parquet or Delta)                                                           |\n",
    "| **PartitionFilters**   | `year = 2024`                                                                                     |\n",
    "| **PushedFilters**      | `completed = true`                                                                                |\n",
    "| **ReadSchema columns** | `event_id`, `user_id`, `duration_sec` *(plus any required metadata columns)*                      |\n",
    "| **Exchange count**     | `0`                                                                                               |\n",
    "| **Assessment**         | Very efficient scan: partition pruning + predicate pushdown + column pruning; no shuffle required |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Join events with artists\n",
    "\n",
    "q2 = events.join(artists, \"artist_id\") \\\n",
    "    .filter(col(\"year\") == 2024) \\\n",
    "    .select(\"event_id\", \"name\", \"genre\", \"duration_sec\")\n",
    "\n",
    "print(\"QUERY 2: Events JOIN Artists (filter after join)\")\n",
    "q2.explain(mode=\"formatted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7741c",
   "metadata": {},
   "source": [
    "| Aspect                 | Your Finding                                                                                                                 |\n",
    "| ---------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Join strategy**      | `BroadcastHashJoin`                                                                                                          |\n",
    "| **Artists table size** | ~5K rows (small!)                                                                                                            |\n",
    "| **Exchange count**     | `0`                                                                                                                          |\n",
    "| **Could broadcast?**   | Yes (artists is well below broadcast threshold)                                                                              |\n",
    "| **Filter placement**   | Applied **after join** (not pushed below join)                                                                               |\n",
    "| **Assessment**         | Join is efficient due to broadcast, but filter placement is suboptimal; filtering `events` before the join would reduce work |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8935f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Three-table join\n",
    "q3 = events.join(tracks, \"track_id\") \\\n",
    "    .join(artists, \"artist_id\") \\\n",
    "    .filter(col(\"year\") == 2024) \\\n",
    "    .filter(col(\"genre\") == \"Pop\") \\\n",
    "    .groupBy(\"name\") \\\n",
    "    .agg(count(\"*\").alias(\"play_count\"), avg(\"duration_sec\").alias(\"avg_duration\"))\n",
    "\n",
    "print(\"QUERY 3: Three-table join with aggregation\")\n",
    "q3.explain(mode=\"formatted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39761df",
   "metadata": {},
   "source": [
    "| Aspect                   | Your Finding                                                                                                         |\n",
    "| ------------------------ | -------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Join 1 strategy**      | `SortMergeJoin` (events ⨝ tracks)                                                                                    |\n",
    "| **Join 2 strategy**      | `BroadcastHashJoin` (artists broadcast)                                                                              |\n",
    "| **Total Exchange count** | `2`                                                                                                                  |\n",
    "| **Filter on year?**      | Yes — **partition filter**, but applied *after joins*                                                                |\n",
    "| **Filter on genre?**     | Yes — **pushed to artists scan**                                                                                     |\n",
    "| **Assessment**           | Correct but expensive plan: shuffle-heavy first join, broadcast second join; filters applied late increase join cost |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c85651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: Aggregation with multiple actions (simulated)\n",
    "enriched = events.join(artists, \"artist_id\").filter(col(\"year\") == 2024)\n",
    "\n",
    "print(\"QUERY 4a: Genre aggregation\")\n",
    "q4a = enriched.groupBy(\"genre\").agg(count(\"*\").alias(\"plays\"))\n",
    "q4a.explain(mode=\"formatted\")\n",
    "\n",
    "print(\"\\nQUERY 4b: Device aggregation (same enriched source)\")\n",
    "q4b = enriched.groupBy(\"device\").agg(avg(\"duration_sec\").alias(\"avg_dur\"))\n",
    "q4b.explain(mode=\"formatted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0455f",
   "metadata": {},
   "source": [
    "| Aspect                                | Your Finding                                                                                |\n",
    "| ------------------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **Does 4a and 4b share computation?** | ❌ No                                                                                        |\n",
    "| **Is enriched cached?**               | ❌ No                                                                                        |\n",
    "| **Redundant work**                    | Join + filter are executed **twice**                                                        |\n",
    "| **Assessment**                        | Inefficient: repeated expensive join and scan; caching `enriched` would avoid recomputation |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf28d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 5: Self-join pattern\n",
    "popular = events.groupBy(\"track_id\").agg(count(\"*\").alias(\"play_count\")) \\\n",
    "    .filter(col(\"play_count\") > 10)\n",
    "\n",
    "q5 = events.join(popular, \"track_id\") \\\n",
    "    .select(\"event_id\", \"user_id\", \"track_id\", \"play_count\")\n",
    "\n",
    "print(\"QUERY 5: Self-reference (events aggregated then joined back)\")\n",
    "q5.explain(mode=\"formatted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a505dbc",
   "metadata": {},
   "source": [
    "| Aspect                                | Your Finding                                                                                               |\n",
    "| ------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n",
    "| **How many times is events scanned?** | **2 times**                                                                                                |\n",
    "| **Exchange count**                    | **2**                                                                                                      |\n",
    "| **Join strategy**                     | `SortMergeJoin`                                                                                            |\n",
    "| **Could caching help?**               | ✅ Yes                                                                                                      |\n",
    "| **Assessment**                        | Correct logic but expensive; double scan + shuffle; caching or restructuring can significantly reduce cost |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5df30",
   "metadata": {},
   "source": [
    "# StreamPulse Execution Plan Audit Report\n",
    "\n",
    "## Summary\n",
    "- **Total queries analyzed:** 5\n",
    "- **Queries needing optimization:** 4 (Queries 2–5)\n",
    "- **Most common issue:** Filters applied *after* joins and repeated recomputation due to missing caching\n",
    "- **Estimated total improvement potential:** 30–60% reduction in shuffle I/O and CPU for complex queries\n",
    "\n",
    "---\n",
    "\n",
    "## Query-by-Query Findings\n",
    "\n",
    "### Query 1: Simple Filter\n",
    "- **Status:** Efficient\n",
    "- **Issues found:** None\n",
    "- **Recommendation:**  \n",
    "  No changes needed; already benefits from partition pruning, predicate pushdown, and column pruning.\n",
    "\n",
    "---\n",
    "\n",
    "### Query 2: Events–Artists Join\n",
    "- **Status:** Needs Work\n",
    "- **Issues found:**\n",
    "  - Filter on `year` applied after join\n",
    "  - Unnecessary rows participate in the join\n",
    "- **Recommendation:**\n",
    "  - Filter `events` on `year = 2024` *before* joining\n",
    "  - Retain broadcast join for `artists`\n",
    "\n",
    "---\n",
    "\n",
    "### Query 3: Three-Table Join with Aggregation\n",
    "- **Status:** Needs Work\n",
    "- **Issues found:**\n",
    "  - Expensive shuffle-based join between `events` and `tracks`\n",
    "  - Partition filter (`year`) applied too late\n",
    "  - Large intermediate datasets before aggregation\n",
    "- **Recommendation:**\n",
    "  - Push filters (`year`, `genre`) to base tables before joins\n",
    "  - Reduce join input size prior to shuffle\n",
    "  - Preserve broadcast join for `artists`\n",
    "\n",
    "---\n",
    "\n",
    "### Query 4: Aggregation with Multiple Actions\n",
    "- **Status:** Needs Work\n",
    "- **Issues found:**\n",
    "  - Same join and filter recomputed for multiple actions\n",
    "  - No shared computation between Query 4a and 4b\n",
    "- **Recommendation:**\n",
    "  - Cache or persist the shared `enriched` DataFrame\n",
    "  - Materialize cache before running multiple aggregations\n",
    "\n",
    "---\n",
    "\n",
    "### Query 5: Self-Join Pattern\n",
    "- **Status:** Needs Work\n",
    "- **Issues found:**\n",
    "  - `events` scanned twice\n",
    "  - Multiple shuffles (aggregation + join)\n",
    "  - SortMergeJoin used even when aggregated side may be small\n",
    "- **Recommendation:**\n",
    "  - Cache the aggregated `popular` DataFrame\n",
    "  - Broadcast `popular` if size permits\n",
    "  - Consider rewriting using window functions to avoid self-join\n",
    "\n",
    "---\n",
    "\n",
    "## Priority Recommendations\n",
    "1. **Push filters as early as possible** (especially partition filters before joins)\n",
    "2. **Cache shared intermediate results** used by multiple actions\n",
    "3. **Leverage broadcast joins explicitly** for small dimension or aggregated tables\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration Recommendations\n",
    "- **spark.sql.autoBroadcastJoinThreshold:**  \n",
    "  Keep default (~10MB) or increase slightly (e.g., 20–50MB) if dimension tables are consistently small\n",
    "\n",
    "- **spark.sql.shuffle.partitions:**  \n",
    "  Tune based on cluster size (e.g., 100–200 instead of default 200 for mid-sized workloads)\n",
    "\n",
    "- **Caching strategy:**  \n",
    "  Cache only reused, expensive intermediates (joins, filtered fact tables)  \n",
    "  Prefer `MEMORY_AND_DISK` for large cached DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, avg, broadcast\n",
    "\n",
    "# -------------------------------\n",
    "# Query 1: Already efficient\n",
    "# -------------------------------\n",
    "q1_optimized = events \\\n",
    "    .filter(col(\"year\") == 2024) \\\n",
    "    .filter(col(\"completed\") == True) \\\n",
    "    .select(\"event_id\", \"user_id\", \"duration_sec\")\n",
    "# No changes needed; partition pruning and column pruning already applied\n",
    "\n",
    "# -------------------------------\n",
    "# Query 2: Push filter before join and broadcast artists\n",
    "# -------------------------------\n",
    "q2_optimized = events \\\n",
    "    .filter(col(\"year\") == 2024) \\\n",
    "    .join(broadcast(artists), \"artist_id\") \\\n",
    "    .select(\"event_id\", \"name\", \"genre\", \"duration_sec\")\n",
    "\n",
    "# -------------------------------\n",
    "# Query 3: Push filters early, preserve broadcast for small artists\n",
    "# -------------------------------\n",
    "events_filtered = events.filter(col(\"year\") == 2024)\n",
    "artists_filtered = artists.filter(col(\"genre\") == \"Pop\")\n",
    "\n",
    "q3_optimized = events_filtered \\\n",
    "    .join(tracks, \"track_id\") \\\n",
    "    .join(broadcast(artists_filtered), \"artist_id\") \\\n",
    "    .groupBy(\"name\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"play_count\"),\n",
    "        avg(\"duration_sec\").alias(\"avg_duration\")\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Query 4: Cache enriched DataFrame for multiple aggregations\n",
    "# -------------------------------\n",
    "enriched = events \\\n",
    "    .join(broadcast(artists), \"artist_id\") \\\n",
    "    .filter(col(\"year\") == 2024) \\\n",
    "    .cache()\n",
    "\n",
    "# Aggregation by genre\n",
    "q4a_optimized = enriched.groupBy(\"genre\") \\\n",
    "    .agg(count(\"*\").alias(\"plays\"))\n",
    "\n",
    "# Aggregation by device\n",
    "q4b_optimized = enriched.groupBy(\"device\") \\\n",
    "    .agg(avg(\"duration_sec\").alias(\"avg_dur\"))\n",
    "\n",
    "# -------------------------------\n",
    "# Query 5: Cache aggregated popular tracks and broadcast if small\n",
    "# -------------------------------\n",
    "popular = events.groupBy(\"track_id\") \\\n",
    "    .agg(count(\"*\").alias(\"play_count\")) \\\n",
    "    .filter(col(\"play_count\") > 10) \\\n",
    "    .cache()  # cache for reuse\n",
    "\n",
    "q5_optimized = events \\\n",
    "    .join(broadcast(popular), \"track_id\") \\\n",
    "    .select(\"event_id\", \"user_id\", \"track_id\", \"play_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57119c0a",
   "metadata": {},
   "source": [
    "✅ Key optimization patterns applied\n",
    "\n",
    "- Filter pushdown: Always filter events or artists early to reduce shuffle and join size\n",
    "\n",
    "- Broadcast joins: Used for small tables (artists) or aggregated tables (popular)\n",
    "\n",
    "- Caching shared intermediate results: enriched (Query 4) and popular (Query 5)\n",
    "\n",
    "- Column pruning: Only select needed columns after joins or filters\n",
    "\n",
    "- Avoid multiple scans: By caching, repeated actions reuse already-computed DataFrames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab18_evnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
