{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e266896a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession created (intentionally misconfigured)\n"
     ]
    }
   ],
   "source": [
    "# set up slow environment \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import random\n",
    "import time\n",
    "import  builtins\n",
    "\n",
    "# ANTI-PATTERN: broadcast disabled, too many shuffle partitions\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StreamPulse-Revenue-SLOW\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ SparkSession created (intentionally misconfigured)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "015e4f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/27 15:40:04 WARN TaskSetManager: Stage 188 contains a task of very large size (4713 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events: 600000 | Subs: 100000 | Ad rates: 48 | Payouts: 5000\n"
     ]
    }
   ],
   "source": [
    "# Generate the revenue dataset:\n",
    "\n",
    "random.seed(42)\n",
    "N = 600000\n",
    "\n",
    "# Events (large)\n",
    "event_data = []\n",
    "for i in range(N):\n",
    "    event_data.append((\n",
    "        f\"EVT-{i+1:07d}\",\n",
    "        f\"USR-{random.randint(1, 100000):06d}\",\n",
    "        f\"ART-{random.randint(1, 5000):05d}\",\n",
    "        random.choice([\"Pop\", \"Rock\", \"Hip-Hop\", \"Jazz\", \"Electronic\", \"R&B\", \"Country\", \"Classical\"]),\n",
    "        random.choice([\"North America\", \"Europe\", \"Asia Pacific\", \"Latin America\", \"Africa\"]),\n",
    "        random.randint(15, 350),\n",
    "        random.choice([True, False]),\n",
    "        random.choice([\"mobile\", \"desktop\", \"smart_speaker\", \"tablet\", \"car\", \"tv\"]),\n",
    "        f\"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d}\",\n",
    "    ))\n",
    "\n",
    "events = spark.createDataFrame(event_data,\n",
    "    [\"event_id\", \"user_id\", \"artist_id\", \"genre\", \"region\",\n",
    "     \"duration_sec\", \"completed\", \"device\", \"event_date\"]) \\\n",
    "    .withColumn(\"event_date\", col(\"event_date\").cast(\"date\")) \\\n",
    "    .withColumn(\"month\", month(col(\"event_date\")))\n",
    "\n",
    "events.write.parquet(\"revenue_data/events\", mode=\"overwrite\")\n",
    "\n",
    "# Subscriptions (medium - 100K users with subscription info)\n",
    "sub_data = [(f\"USR-{i+1:06d}\",\n",
    "             random.choice([\"free\", \"individual\", \"family\", \"student\"]),\n",
    "             builtins.round(random.choice([0.0, 9.99, 14.99, 4.99]), 2),\n",
    "             random.choice([\"US\", \"UK\", \"DE\", \"JP\", \"BR\", \"IN\", \"KR\", \"FR\"]))\n",
    "            for i in range(100000)]\n",
    "subscriptions = spark.createDataFrame(sub_data, [\"user_id\", \"plan\", \"monthly_price\", \"country\"])\n",
    "subscriptions.write.parquet(\"revenue_data/subscriptions\", mode=\"overwrite\")\n",
    "\n",
    "# Ad rates (tiny - 8 genres x 6 devices = 48 rows)\n",
    "ad_data = []\n",
    "for genre in [\"Pop\", \"Rock\", \"Hip-Hop\", \"Jazz\", \"Electronic\", \"R&B\", \"Country\", \"Classical\"]:\n",
    "    for device in [\"mobile\", \"desktop\", \"smart_speaker\", \"tablet\", \"car\", \"tv\"]:\n",
    "        cpm = builtins.round(random.uniform(1.5, 8.0), 2)\n",
    "        ad_data.append((genre, device, cpm))\n",
    "ad_rates = spark.createDataFrame(ad_data, [\"ad_genre\", \"ad_device\", \"cpm\"])\n",
    "ad_rates.write.parquet(\"revenue_data/ad_rates\", mode=\"overwrite\")\n",
    "\n",
    "# Artist payout rates (small - 5000 artists)\n",
    "payout_data = [(f\"ART-{i+1:05d}\", builtins.round(random.uniform(0.003, 0.008), 4),\n",
    "                random.choice([\"major\", \"indie\", \"unsigned\"]))\n",
    "               for i in range(5000)]\n",
    "payouts = spark.createDataFrame(payout_data, [\"artist_id\", \"per_stream_rate\", \"label_type\"])\n",
    "payouts.write.parquet(\"revenue_data/payouts\", mode=\"overwrite\")\n",
    "\n",
    "# Reload from disk\n",
    "events = spark.read.parquet(\"revenue_data/events\")\n",
    "subscriptions = spark.read.parquet(\"revenue_data/subscriptions\")\n",
    "ad_rates = spark.read.parquet(\"revenue_data/ad_rates\")\n",
    "payouts = spark.read.parquet(\"revenue_data/payouts\")\n",
    "\n",
    "print(f\"Events: {events.count()} | Subs: {subscriptions.count()} | \"\n",
    "      f\"Ad rates: {ad_rates.count()} | Payouts: {payouts.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6895f2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING UNOPTIMIZED PIPELINE (BASELINE)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:===============================>                      (115 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report 1 (genre):        10.83s\n",
      "Report 2 (regional):     5.16s\n",
      "Report 3 (subscription): 8.64s\n",
      "Report 4 (ad perf):      5.50s\n",
      "Report 5 (payouts):      3.48s\n",
      "Report 6 (daily):        11.67s\n",
      "\n",
      "⏱️  BASELINE TOTAL: 45.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Part 2: The Unoptimized Pipeline (Baseline)\n",
    "# Run the slow pipeline and time it:\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING UNOPTIMIZED PIPELINE (BASELINE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "# Build enriched revenue DataFrame (NOT cached, recomputed every time)\n",
    "def build_revenue():\n",
    "    return events \\\n",
    "        .join(subscriptions, \"user_id\") \\\n",
    "        .join(ad_rates,\n",
    "              (events.genre == ad_rates.ad_genre) & (events.device == ad_rates.ad_device)) \\\n",
    "        .join(payouts, \"artist_id\") \\\n",
    "        .withColumn(\"ad_revenue\", col(\"cpm\") / 1000) \\\n",
    "        .withColumn(\"stream_payout\", col(\"per_stream_rate\")) \\\n",
    "        .withColumn(\"is_premium\", when(col(\"plan\") != \"free\", True).otherwise(False))\n",
    "\n",
    "# Report 1: Genre Revenue\n",
    "revenue = build_revenue()\n",
    "r1_start = time.time()\n",
    "report_1 = revenue.groupBy(\"genre\") \\\n",
    "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
    "         countDistinct(\"user_id\").alias(\"unique_listeners\")) \\\n",
    "    .collect()\n",
    "r1_time = time.time() - r1_start\n",
    "\n",
    "# Report 2: Regional Breakdown\n",
    "revenue = build_revenue()\n",
    "r2_start = time.time()\n",
    "report_2 = revenue.groupBy(\"region\", \"country\") \\\n",
    "    .agg(count(\"*\").alias(\"streams\"),\n",
    "         sum(\"ad_revenue\").alias(\"ad_rev\")) \\\n",
    "    .collect()\n",
    "r2_time = time.time() - r2_start\n",
    "\n",
    "# Report 3: Subscription Analysis\n",
    "revenue = build_revenue()\n",
    "r3_start = time.time()\n",
    "report_3 = revenue.groupBy(\"plan\") \\\n",
    "    .agg(countDistinct(\"user_id\").alias(\"users\"),\n",
    "         count(\"*\").alias(\"total_streams\"),\n",
    "         avg(\"duration_sec\").alias(\"avg_duration\")) \\\n",
    "    .collect()\n",
    "r3_time = time.time() - r3_start\n",
    "\n",
    "# Report 4: Ad Performance\n",
    "revenue = build_revenue()\n",
    "r4_start = time.time()\n",
    "report_4 = revenue.groupBy(\"device\", \"genre\") \\\n",
    "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
    "         count(\"*\").alias(\"impressions\")) \\\n",
    "    .collect()\n",
    "r4_time = time.time() - r4_start\n",
    "\n",
    "# Report 5: Artist Payouts\n",
    "revenue = build_revenue()\n",
    "r5_start = time.time()\n",
    "report_5 = revenue.groupBy(\"artist_id\", \"label_type\") \\\n",
    "    .agg(sum(\"stream_payout\").alias(\"total_payout\"),\n",
    "         count(\"*\").alias(\"total_streams\")) \\\n",
    "    .orderBy(desc(\"total_payout\")).limit(100) \\\n",
    "    .collect()\n",
    "r5_time = time.time() - r5_start\n",
    "\n",
    "# Report 6: Daily Summary\n",
    "revenue = build_revenue()\n",
    "r6_start = time.time()\n",
    "report_6 = revenue.groupBy(\"event_date\") \\\n",
    "    .agg(count(\"*\").alias(\"streams\"),\n",
    "         sum(\"ad_revenue\").alias(\"ad_rev\"),\n",
    "         countDistinct(\"user_id\").alias(\"unique_users\")) \\\n",
    "    .orderBy(\"event_date\") \\\n",
    "    .collect()\n",
    "r6_time = time.time() - r6_start\n",
    "\n",
    "baseline_total = time.time() - total_start\n",
    "\n",
    "print(f\"\\nReport 1 (genre):        {r1_time:.2f}s\")\n",
    "print(f\"Report 2 (regional):     {r2_time:.2f}s\")\n",
    "print(f\"Report 3 (subscription): {r3_time:.2f}s\")\n",
    "print(f\"Report 4 (ad perf):      {r4_time:.2f}s\")\n",
    "print(f\"Report 5 (payouts):      {r5_time:.2f}s\")\n",
    "print(f\"Report 6 (daily):        {r6_time:.2f}s\")\n",
    "print(f\"\\n⏱️  BASELINE TOTAL: {baseline_total:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "035ef421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BASELINE PLAN:\n",
      "== Physical Plan ==\n",
      "* HashAggregate (33)\n",
      "+- Exchange (32)\n",
      "   +- * HashAggregate (31)\n",
      "      +- * Project (30)\n",
      "         +- * SortMergeJoin Inner (29)\n",
      "            :- * Sort (23)\n",
      "            :  +- Exchange (22)\n",
      "            :     +- * Project (21)\n",
      "            :        +- * SortMergeJoin Inner (20)\n",
      "            :           :- * Sort (14)\n",
      "            :           :  +- Exchange (13)\n",
      "            :           :     +- * Project (12)\n",
      "            :           :        +- * SortMergeJoin Inner (11)\n",
      "            :           :           :- * Sort (5)\n",
      "            :           :           :  +- Exchange (4)\n",
      "            :           :           :     +- * Filter (3)\n",
      "            :           :           :        +- * ColumnarToRow (2)\n",
      "            :           :           :           +- Scan parquet  (1)\n",
      "            :           :           +- * Sort (10)\n",
      "            :           :              +- Exchange (9)\n",
      "            :           :                 +- * Filter (8)\n",
      "            :           :                    +- * ColumnarToRow (7)\n",
      "            :           :                       +- Scan parquet  (6)\n",
      "            :           +- * Sort (19)\n",
      "            :              +- Exchange (18)\n",
      "            :                 +- * Filter (17)\n",
      "            :                    +- * ColumnarToRow (16)\n",
      "            :                       +- Scan parquet  (15)\n",
      "            +- * Sort (28)\n",
      "               +- Exchange (27)\n",
      "                  +- * Filter (26)\n",
      "                     +- * ColumnarToRow (25)\n",
      "                        +- Scan parquet  (24)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [4]: [user_id#260, artist_id#261, genre#262, device#266]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/events]\n",
      "PushedFilters: [IsNotNull(user_id), IsNotNull(genre), IsNotNull(device), IsNotNull(artist_id)]\n",
      "ReadSchema: struct<user_id:string,artist_id:string,genre:string,device:string>\n",
      "\n",
      "(2) ColumnarToRow [codegen id : 1]\n",
      "Input [4]: [user_id#260, artist_id#261, genre#262, device#266]\n",
      "\n",
      "(3) Filter [codegen id : 1]\n",
      "Input [4]: [user_id#260, artist_id#261, genre#262, device#266]\n",
      "Condition : (((isnotnull(user_id#260) AND isnotnull(genre#262)) AND isnotnull(device#266)) AND isnotnull(artist_id#261))\n",
      "\n",
      "(4) Exchange\n",
      "Input [4]: [user_id#260, artist_id#261, genre#262, device#266]\n",
      "Arguments: hashpartitioning(user_id#260, 200), ENSURE_REQUIREMENTS, [plan_id=2144]\n",
      "\n",
      "(5) Sort [codegen id : 2]\n",
      "Input [4]: [user_id#260, artist_id#261, genre#262, device#266]\n",
      "Arguments: [user_id#260 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(6) Scan parquet \n",
      "Output [1]: [user_id#279]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/subscriptions]\n",
      "PushedFilters: [IsNotNull(user_id)]\n",
      "ReadSchema: struct<user_id:string>\n",
      "\n",
      "(7) ColumnarToRow [codegen id : 3]\n",
      "Input [1]: [user_id#279]\n",
      "\n",
      "(8) Filter [codegen id : 3]\n",
      "Input [1]: [user_id#279]\n",
      "Condition : isnotnull(user_id#279)\n",
      "\n",
      "(9) Exchange\n",
      "Input [1]: [user_id#279]\n",
      "Arguments: hashpartitioning(user_id#279, 200), ENSURE_REQUIREMENTS, [plan_id=2153]\n",
      "\n",
      "(10) Sort [codegen id : 4]\n",
      "Input [1]: [user_id#279]\n",
      "Arguments: [user_id#279 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(11) SortMergeJoin [codegen id : 5]\n",
      "Left keys [1]: [user_id#260]\n",
      "Right keys [1]: [user_id#279]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(12) Project [codegen id : 5]\n",
      "Output [3]: [artist_id#261, genre#262, device#266]\n",
      "Input [5]: [user_id#260, artist_id#261, genre#262, device#266, user_id#279]\n",
      "\n",
      "(13) Exchange\n",
      "Input [3]: [artist_id#261, genre#262, device#266]\n",
      "Arguments: hashpartitioning(genre#262, device#266, 200), ENSURE_REQUIREMENTS, [plan_id=2161]\n",
      "\n",
      "(14) Sort [codegen id : 6]\n",
      "Input [3]: [artist_id#261, genre#262, device#266]\n",
      "Arguments: [genre#262 ASC NULLS FIRST, device#266 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(15) Scan parquet \n",
      "Output [3]: [ad_genre#287, ad_device#288, cpm#289]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/ad_rates]\n",
      "PushedFilters: [IsNotNull(ad_genre), IsNotNull(ad_device)]\n",
      "ReadSchema: struct<ad_genre:string,ad_device:string,cpm:double>\n",
      "\n",
      "(16) ColumnarToRow [codegen id : 7]\n",
      "Input [3]: [ad_genre#287, ad_device#288, cpm#289]\n",
      "\n",
      "(17) Filter [codegen id : 7]\n",
      "Input [3]: [ad_genre#287, ad_device#288, cpm#289]\n",
      "Condition : (isnotnull(ad_genre#287) AND isnotnull(ad_device#288))\n",
      "\n",
      "(18) Exchange\n",
      "Input [3]: [ad_genre#287, ad_device#288, cpm#289]\n",
      "Arguments: hashpartitioning(ad_genre#287, ad_device#288, 200), ENSURE_REQUIREMENTS, [plan_id=2170]\n",
      "\n",
      "(19) Sort [codegen id : 8]\n",
      "Input [3]: [ad_genre#287, ad_device#288, cpm#289]\n",
      "Arguments: [ad_genre#287 ASC NULLS FIRST, ad_device#288 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(20) SortMergeJoin [codegen id : 9]\n",
      "Left keys [2]: [genre#262, device#266]\n",
      "Right keys [2]: [ad_genre#287, ad_device#288]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(21) Project [codegen id : 9]\n",
      "Output [3]: [artist_id#261, genre#262, cpm#289]\n",
      "Input [6]: [artist_id#261, genre#262, device#266, ad_genre#287, ad_device#288, cpm#289]\n",
      "\n",
      "(22) Exchange\n",
      "Input [3]: [artist_id#261, genre#262, cpm#289]\n",
      "Arguments: hashpartitioning(artist_id#261, 200), ENSURE_REQUIREMENTS, [plan_id=2178]\n",
      "\n",
      "(23) Sort [codegen id : 10]\n",
      "Input [3]: [artist_id#261, genre#262, cpm#289]\n",
      "Arguments: [artist_id#261 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(24) Scan parquet \n",
      "Output [1]: [artist_id#293]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/payouts]\n",
      "PushedFilters: [IsNotNull(artist_id)]\n",
      "ReadSchema: struct<artist_id:string>\n",
      "\n",
      "(25) ColumnarToRow [codegen id : 11]\n",
      "Input [1]: [artist_id#293]\n",
      "\n",
      "(26) Filter [codegen id : 11]\n",
      "Input [1]: [artist_id#293]\n",
      "Condition : isnotnull(artist_id#293)\n",
      "\n",
      "(27) Exchange\n",
      "Input [1]: [artist_id#293]\n",
      "Arguments: hashpartitioning(artist_id#293, 200), ENSURE_REQUIREMENTS, [plan_id=2187]\n",
      "\n",
      "(28) Sort [codegen id : 12]\n",
      "Input [1]: [artist_id#293]\n",
      "Arguments: [artist_id#293 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(29) SortMergeJoin [codegen id : 13]\n",
      "Left keys [1]: [artist_id#261]\n",
      "Right keys [1]: [artist_id#293]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(30) Project [codegen id : 13]\n",
      "Output [2]: [genre#262, (cpm#289 / 1000.0) AS ad_revenue#1404]\n",
      "Input [4]: [artist_id#261, genre#262, cpm#289, artist_id#293]\n",
      "\n",
      "(31) HashAggregate [codegen id : 13]\n",
      "Input [2]: [genre#262, ad_revenue#1404]\n",
      "Keys [1]: [genre#262]\n",
      "Functions [1]: [partial_sum(ad_revenue#1404)]\n",
      "Aggregate Attributes [1]: [sum#1492]\n",
      "Results [2]: [genre#262, sum#1493]\n",
      "\n",
      "(32) Exchange\n",
      "Input [2]: [genre#262, sum#1493]\n",
      "Arguments: hashpartitioning(genre#262, 200), ENSURE_REQUIREMENTS, [plan_id=2196]\n",
      "\n",
      "(33) HashAggregate [codegen id : 14]\n",
      "Input [2]: [genre#262, sum#1493]\n",
      "Keys [1]: [genre#262]\n",
      "Functions [1]: [sum(ad_revenue#1404)]\n",
      "Aggregate Attributes [1]: [sum(ad_revenue#1404)#1488]\n",
      "Results [2]: [genre#262, sum(ad_revenue#1404)#1488 AS sum(ad_revenue)#1489]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze the baseline plan:\n",
    "print(\"\\nBASELINE PLAN:\")\n",
    "build_revenue().groupBy(\"genre\").agg(sum(\"ad_revenue\")).explain(mode=\"formatted\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7abc375",
   "metadata": {},
   "source": [
    "| Anti-Pattern | Description                                              | Impact                                                    |\n",
    "| ------------ | -------------------------------------------------------- | --------------------------------------------------------- |\n",
    "| 1            | Rebuilding the same joined DataFrame for every report    | Repeats expensive joins and shuffles six times            |\n",
    "| 2            | No caching or persistence of shared intermediate results | Prevents reuse; Spark recomputes lineage for every action |\n",
    "| 3            | Multiple `.collect()` calls                              | Forces full execution and driver memory pressure          |\n",
    "| 4            | Large fact table (`events`) joined repeatedly            | Dominates runtime and I/O cost                            |\n",
    "| 5            | No broadcast joins for small dimension tables            | Causes unnecessary shuffles and SortMergeJoins            |\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986db08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tells Spark:\n",
    "#If a table is ≤ 10 MB, automatically broadcast it to all executors instead of shuffling.”\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")\n",
    "# Your optimized code here — use broadcast() for ad_rates and payouts\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "def build_revenue_optimized():\n",
    "    return events \\\n",
    "        .join(broadcast(subscriptions), \"user_id\") \\\n",
    "        .join(\n",
    "            broadcast(ad_rates),\n",
    "            (events.genre == ad_rates.ad_genre) &\n",
    "            (events.device == ad_rates.ad_device)\n",
    "        ) \\\n",
    "        .join(broadcast(payouts), \"artist_id\") \\\n",
    "        .withColumn(\"ad_revenue\", col(\"cpm\") / 1000) \\\n",
    "        .withColumn(\"stream_payout\", col(\"per_stream_rate\")) \\\n",
    "        .withColumn(\"is_premium\", when(col(\"plan\") != \"free\", True).otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0e173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING OPTIMIZED PIPELINE \n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report 1 (genre):        2.44s\n",
      "Report 2 (regional):     1.01s\n",
      "Report 3 (subscription): 1.77s\n",
      "Report 4 (ad perf):      0.80s\n",
      "Report 5 (payouts):      0.88s\n",
      "Report 6 (daily):        5.01s\n",
      "\n",
      "⏱️  OPTIMIZED TOTAL: 12.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RUNNING OPTIMIZED BROADCAST PIPELINE \")\n",
    "print(\"=\" * 60)\n",
    " \n",
    "total_start = time.time()\n",
    "\n",
    "# Report 1 : Genre Revenue\n",
    "revenue = build_revenue_optimized()\n",
    "r1_start = time.time()\n",
    "report_1 = revenue.groupBy(\"genre\") \\\n",
    "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
    "         countDistinct(\"user_id\").alias(\"unique_listeners\")) \\\n",
    "    .collect()\n",
    "r1_time = time.time() - r1_start\n",
    "\n",
    "# Report 2: Regional Breakdown\n",
    "revenue = build_revenue_optimized()\n",
    "r2_start = time.time()\n",
    "report_2 = revenue.groupBy(\"region\", \"country\") \\\n",
    "    .agg(count(\"*\").alias(\"streams\"),\n",
    "         sum(\"ad_revenue\").alias(\"ad_rev\")) \\\n",
    "    .collect()\n",
    "r2_time = time.time() - r2_start\n",
    " \n",
    "# Report 3: Subscription Analysis\n",
    "revenue = build_revenue_optimized()\n",
    "r3_start = time.time()\n",
    "report_3 = revenue.groupBy(\"plan\") \\\n",
    "    .agg(countDistinct(\"user_id\").alias(\"users\"),\n",
    "         count(\"*\").alias(\"total_streams\"),\n",
    "         avg(\"duration_sec\").alias(\"avg_duration\")) \\\n",
    "    .collect()\n",
    "r3_time = time.time() - r3_start\n",
    " \n",
    "# Report 4: Ad Performance\n",
    "revenue = build_revenue_optimized()\n",
    "r4_start = time.time()\n",
    "report_4 = revenue.groupBy(\"device\", \"genre\") \\\n",
    "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
    "         count(\"*\").alias(\"impressions\")) \\\n",
    "    .collect()\n",
    "r4_time = time.time() - r4_start\n",
    " \n",
    "# Report 5: Artist Payouts\n",
    "revenue = build_revenue_optimized()\n",
    "r5_start = time.time()\n",
    "report_5 = revenue.groupBy(\"artist_id\", \"label_type\") \\\n",
    "    .agg(sum(\"stream_payout\").alias(\"total_payout\"),\n",
    "         count(\"*\").alias(\"total_streams\")) \\\n",
    "    .orderBy(desc(\"total_payout\")).limit(100) \\\n",
    "    .collect()\n",
    "r5_time = time.time() - r5_start\n",
    " \n",
    "# Report 6: Daily Summary\n",
    "revenue = build_revenue_optimized()\n",
    "r6_start = time.time()\n",
    "report_6 = revenue.groupBy(\"event_date\") \\\n",
    "    .agg(count(\"*\").alias(\"streams\"),\n",
    "         sum(\"ad_revenue\").alias(\"ad_rev\"),\n",
    "         countDistinct(\"user_id\").alias(\"unique_users\")) \\\n",
    "    .orderBy(\"event_date\") \\\n",
    "    .collect()\n",
    "r6_time = time.time() - r6_start\n",
    " \n",
    "optimized_total = time.time() - total_start\n",
    " \n",
    "print(f\"\\nReport 1 (genre):        {r1_time:.2f}s\")\n",
    "print(f\"Report 2 (regional):     {r2_time:.2f}s\")\n",
    "print(f\"Report 3 (subscription): {r3_time:.2f}s\")\n",
    "print(f\"Report 4 (ad perf):      {r4_time:.2f}s\")\n",
    "print(f\"Report 5 (payouts):      {r5_time:.2f}s\")\n",
    "print(f\"Report 6 (daily):        {r6_time:.2f}s\")\n",
    "print(f\"\\n⏱️  OPTIMIZED BROADCAST TOTAL: {optimized_total:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b0e2f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* HashAggregate (24)\n",
      "+- Exchange (23)\n",
      "   +- * HashAggregate (22)\n",
      "      +- * Project (21)\n",
      "         +- * BroadcastHashJoin Inner BuildRight (20)\n",
      "            :- * Project (15)\n",
      "            :  +- * BroadcastHashJoin Inner BuildRight (14)\n",
      "            :     :- * Project (9)\n",
      "            :     :  +- * BroadcastHashJoin Inner BuildRight (8)\n",
      "            :     :     :- * Filter (3)\n",
      "            :     :     :  +- * ColumnarToRow (2)\n",
      "            :     :     :     +- Scan parquet  (1)\n",
      "            :     :     +- BroadcastExchange (7)\n",
      "            :     :        +- * Filter (6)\n",
      "            :     :           +- * ColumnarToRow (5)\n",
      "            :     :              +- Scan parquet  (4)\n",
      "            :     +- BroadcastExchange (13)\n",
      "            :        +- * Filter (12)\n",
      "            :           +- * ColumnarToRow (11)\n",
      "            :              +- Scan parquet  (10)\n",
      "            +- BroadcastExchange (19)\n",
      "               +- * Filter (18)\n",
      "                  +- * ColumnarToRow (17)\n",
      "                     +- Scan parquet  (16)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [4]: [user_id#260, artist_id#261, genre#262, device#266]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/events]\n",
      "PushedFilters: [IsNotNull(user_id), IsNotNull(genre), IsNotNull(device), IsNotNull(artist_id)]\n",
      "ReadSchema: struct<user_id:string,artist_id:string,genre:string,device:string>\n",
      "\n",
      "(2) ColumnarToRow [codegen id : 4]\n",
      "Input [4]: [user_id#260, artist_id#261, genre#262, device#266]\n",
      "\n",
      "(3) Filter [codegen id : 4]\n",
      "Input [4]: [user_id#260, artist_id#261, genre#262, device#266]\n",
      "Condition : (((isnotnull(user_id#260) AND isnotnull(genre#262)) AND isnotnull(device#266)) AND isnotnull(artist_id#261))\n",
      "\n",
      "(4) Scan parquet \n",
      "Output [1]: [user_id#279]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/subscriptions]\n",
      "PushedFilters: [IsNotNull(user_id)]\n",
      "ReadSchema: struct<user_id:string>\n",
      "\n",
      "(5) ColumnarToRow [codegen id : 1]\n",
      "Input [1]: [user_id#279]\n",
      "\n",
      "(6) Filter [codegen id : 1]\n",
      "Input [1]: [user_id#279]\n",
      "Condition : isnotnull(user_id#279)\n",
      "\n",
      "(7) BroadcastExchange\n",
      "Input [1]: [user_id#279]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=2581]\n",
      "\n",
      "(8) BroadcastHashJoin [codegen id : 4]\n",
      "Left keys [1]: [user_id#260]\n",
      "Right keys [1]: [user_id#279]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(9) Project [codegen id : 4]\n",
      "Output [3]: [artist_id#261, genre#262, device#266]\n",
      "Input [5]: [user_id#260, artist_id#261, genre#262, device#266, user_id#279]\n",
      "\n",
      "(10) Scan parquet \n",
      "Output [3]: [ad_genre#287, ad_device#288, cpm#289]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/ad_rates]\n",
      "PushedFilters: [IsNotNull(ad_genre), IsNotNull(ad_device)]\n",
      "ReadSchema: struct<ad_genre:string,ad_device:string,cpm:double>\n",
      "\n",
      "(11) ColumnarToRow [codegen id : 2]\n",
      "Input [3]: [ad_genre#287, ad_device#288, cpm#289]\n",
      "\n",
      "(12) Filter [codegen id : 2]\n",
      "Input [3]: [ad_genre#287, ad_device#288, cpm#289]\n",
      "Condition : (isnotnull(ad_genre#287) AND isnotnull(ad_device#288))\n",
      "\n",
      "(13) BroadcastExchange\n",
      "Input [3]: [ad_genre#287, ad_device#288, cpm#289]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false], input[1, string, false]),false), [plan_id=2589]\n",
      "\n",
      "(14) BroadcastHashJoin [codegen id : 4]\n",
      "Left keys [2]: [genre#262, device#266]\n",
      "Right keys [2]: [ad_genre#287, ad_device#288]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(15) Project [codegen id : 4]\n",
      "Output [3]: [artist_id#261, genre#262, cpm#289]\n",
      "Input [6]: [artist_id#261, genre#262, device#266, ad_genre#287, ad_device#288, cpm#289]\n",
      "\n",
      "(16) Scan parquet \n",
      "Output [1]: [artist_id#293]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/payouts]\n",
      "PushedFilters: [IsNotNull(artist_id)]\n",
      "ReadSchema: struct<artist_id:string>\n",
      "\n",
      "(17) ColumnarToRow [codegen id : 3]\n",
      "Input [1]: [artist_id#293]\n",
      "\n",
      "(18) Filter [codegen id : 3]\n",
      "Input [1]: [artist_id#293]\n",
      "Condition : isnotnull(artist_id#293)\n",
      "\n",
      "(19) BroadcastExchange\n",
      "Input [1]: [artist_id#293]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=2597]\n",
      "\n",
      "(20) BroadcastHashJoin [codegen id : 4]\n",
      "Left keys [1]: [artist_id#261]\n",
      "Right keys [1]: [artist_id#293]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(21) Project [codegen id : 4]\n",
      "Output [2]: [genre#262, (cpm#289 / 1000.0) AS ad_revenue#1712]\n",
      "Input [4]: [artist_id#261, genre#262, cpm#289, artist_id#293]\n",
      "\n",
      "(22) HashAggregate [codegen id : 4]\n",
      "Input [2]: [genre#262, ad_revenue#1712]\n",
      "Keys [1]: [genre#262]\n",
      "Functions [1]: [partial_sum(ad_revenue#1712)]\n",
      "Aggregate Attributes [1]: [sum#1800]\n",
      "Results [2]: [genre#262, sum#1801]\n",
      "\n",
      "(23) Exchange\n",
      "Input [2]: [genre#262, sum#1801]\n",
      "Arguments: hashpartitioning(genre#262, 200), ENSURE_REQUIREMENTS, [plan_id=2603]\n",
      "\n",
      "(24) HashAggregate [codegen id : 5]\n",
      "Input [2]: [genre#262, sum#1801]\n",
      "Keys [1]: [genre#262]\n",
      "Functions [1]: [sum(ad_revenue#1712)]\n",
      "Aggregate Attributes [1]: [sum(ad_revenue#1712)#1796]\n",
      "Results [2]: [genre#262, sum(ad_revenue#1712)#1796 AS sum(ad_revenue)#1797]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Verifying the optimization with explain()\n",
    "\n",
    "build_revenue_optimized() \\\n",
    "    .groupBy(\"genre\") \\\n",
    "    .agg(sum(\"ad_revenue\")) \\\n",
    "    .explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bfd3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING BROADCAST + CACHED PIPELINE \n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/27 15:21:10 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cached filtered+joined Revenue DataFrame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 165:==================================>                  (131 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report 1 (genre):        1.25s\n",
      "Report 2 (regional):     0.25s\n",
      "Report 3 (subscription): 1.04s\n",
      "Report 4 (ad perf):      0.29s\n",
      "Report 5 (payouts):      0.42s\n",
      "Report 6 (daily):        4.14s\n",
      "\n",
      "⏱️  OPTIMIZED BROADCAST + CACHHED TOTAL: 7.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Optimization 2: Cache the enriched Data Frame\n",
    "# Build once, cache, reuse for all 6 reports\n",
    "# Measure and compare\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING BROADCAST + CACHED PIPELINE \")\n",
    "print(\"=\" * 60)\n",
    " \n",
    "total_start = time.time()\n",
    "\n",
    "def build_revenue_cached():\n",
    "    return events \\\n",
    "        .join(broadcast(subscriptions), \"user_id\") \\\n",
    "        .join(\n",
    "            broadcast(ad_rates),\n",
    "            (events.genre == ad_rates.ad_genre) &\n",
    "            (events.device == ad_rates.ad_device)\n",
    "        ) \\\n",
    "        .join(broadcast(payouts), \"artist_id\") \\\n",
    "        .withColumn(\"ad_revenue\", col(\"cpm\") / 1000) \\\n",
    "        .withColumn(\"stream_payout\", col(\"per_stream_rate\")) \\\n",
    "        .withColumn(\"is_premium\", when(col(\"plan\") != \"free\", True).otherwise(False))\n",
    "        \n",
    "# build once \n",
    "revenue = build_revenue_cached()\n",
    "# Cache it\n",
    "revenue.cache()\n",
    "# trigger/ materialize cache population\n",
    "revenue.count()\n",
    "print(\"✅ Cached filtered+joined Revenue DataFrame\")\n",
    "\n",
    "# Step 3: Reuse cached DataFrame for all 6 reports\n",
    "# Report 1: Genre Revenue\n",
    "r1_start = time.time()\n",
    "report_1 = revenue.groupBy(\"genre\") \\\n",
    "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
    "         countDistinct(\"user_id\").alias(\"unique_listeners\")) \\\n",
    "    .collect()\n",
    "r1_time = time.time() - r1_start\n",
    " \n",
    "# Report 2: Regional Breakdown\n",
    "r2_start = time.time()\n",
    "report_2 = revenue.groupBy(\"region\", \"country\") \\\n",
    "    .agg(count(\"*\").alias(\"streams\"),\n",
    "         sum(\"ad_revenue\").alias(\"ad_rev\")) \\\n",
    "    .collect()\n",
    "r2_time = time.time() - r2_start\n",
    " \n",
    "# Report 3: Subscription Analysis\n",
    "r3_start = time.time()\n",
    "report_3 = revenue.groupBy(\"plan\") \\\n",
    "    .agg(countDistinct(\"user_id\").alias(\"users\"),\n",
    "         count(\"*\").alias(\"total_streams\"),\n",
    "         avg(\"duration_sec\").alias(\"avg_duration\")) \\\n",
    "    .collect()\n",
    "r3_time = time.time() - r3_start\n",
    " \n",
    "# Report 4: Ad Performance\n",
    "r4_start = time.time()\n",
    "report_4 = revenue.groupBy(\"device\", \"genre\") \\\n",
    "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
    "         count(\"*\").alias(\"impressions\")) \\\n",
    "    .collect()\n",
    "r4_time = time.time() - r4_start\n",
    " \n",
    "# Report 5: Artist Payouts\n",
    "r5_start = time.time()\n",
    "report_5 = revenue.groupBy(\"artist_id\", \"label_type\") \\\n",
    "    .agg(sum(\"stream_payout\").alias(\"total_payout\"),\n",
    "         count(\"*\").alias(\"total_streams\")) \\\n",
    "    .orderBy(desc(\"total_payout\")).limit(100) \\\n",
    "    .collect()\n",
    "r5_time = time.time() - r5_start\n",
    " \n",
    "# Report 6: Daily Summary\n",
    "r6_start = time.time()\n",
    "report_6 = revenue.groupBy(\"event_date\") \\\n",
    "    .agg(count(\"*\").alias(\"streams\"),\n",
    "         sum(\"ad_revenue\").alias(\"ad_rev\"),\n",
    "         countDistinct(\"user_id\").alias(\"unique_users\")) \\\n",
    "    .orderBy(\"event_date\") \\\n",
    "    .collect()\n",
    "r6_time = time.time() - r6_start\n",
    " \n",
    "cached_total = time.time() - total_start\n",
    " \n",
    "print(f\"\\nReport 1 (genre):        {r1_time:.2f}s\")\n",
    "print(f\"Report 2 (regional):     {r2_time:.2f}s\")\n",
    "print(f\"Report 3 (subscription): {r3_time:.2f}s\")\n",
    "print(f\"Report 4 (ad perf):      {r4_time:.2f}s\")\n",
    "print(f\"Report 5 (payouts):      {r5_time:.2f}s\")\n",
    "print(f\"Report 6 (daily):        {r6_time:.2f}s\")\n",
    "print(f\"\\n⏱️  OPTIMIZED BROADCAST + CACHHED TOTAL: {cached_total:.2f}s\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a35d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare cached optimization worked \n",
    "revenue.groupBy(\"genre\")\\\n",
    "    .agg(sum(\"ad_revenue\"))\\\n",
    "        .explain(mode = \"formatted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d8728de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING BROADCAST + CACHED + REDUCE SHUFFLE PIPELINE \n",
      "============================================================\n",
      "✅ Cached filtered+joined Revenue DataFrame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/27 15:29:03 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report 1 (genre):        0.46s\n",
      "Report 2 (regional):     0.18s\n",
      "Report 3 (subscription): 0.20s\n",
      "Report 4 (ad perf):      0.10s\n",
      "Report 5 (payouts):      0.18s\n",
      "Report 6 (daily):        0.30s\n",
      "\n",
      "⏱️  OPTIMIZED BROADCAST + CACHHED + REDUCE SHUFFLE TOTAL: 1.56s\n"
     ]
    }
   ],
   "source": [
    "#Optimization 3: Reduce shuffle partitions\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING BROADCAST + CACHED + REDUCE SHUFFLE PIPELINE \")\n",
    "print(\"=\" * 60)\n",
    " \n",
    "total_start = time.time()\n",
    "\n",
    "def build_revenue_cached():\n",
    "    return events \\\n",
    "        .join(broadcast(subscriptions), \"user_id\") \\\n",
    "        .join(\n",
    "            broadcast(ad_rates),\n",
    "            (events.genre == ad_rates.ad_genre) &\n",
    "            (events.device == ad_rates.ad_device)\n",
    "        ) \\\n",
    "        .join(broadcast(payouts), \"artist_id\") \\\n",
    "        .withColumn(\"ad_revenue\", col(\"cpm\") / 1000) \\\n",
    "        .withColumn(\"stream_payout\", col(\"per_stream_rate\")) \\\n",
    "        .withColumn(\"is_premium\", when(col(\"plan\") != \"free\", True).otherwise(False))\n",
    "        \n",
    "# build once \n",
    "revenue = build_revenue_cached()\n",
    "# Cache it\n",
    "revenue.cache()\n",
    "# trigger/ materialize cache population\n",
    "revenue.count()\n",
    "print(\"✅ Cached filtered+joined Revenue DataFrame\")\n",
    "\n",
    "# Step 3: Reuse cached DataFrame for all 6 reports\n",
    "# Report 1: Genre Revenue\n",
    "r1_start = time.time()\n",
    "report_1 = revenue.groupBy(\"genre\") \\\n",
    "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
    "         countDistinct(\"user_id\").alias(\"unique_listeners\")) \\\n",
    "    .collect()\n",
    "r1_time = time.time() - r1_start\n",
    " \n",
    "# Report 2: Regional Breakdown\n",
    "r2_start = time.time()\n",
    "report_2 = revenue.groupBy(\"region\", \"country\") \\\n",
    "    .agg(count(\"*\").alias(\"streams\"),\n",
    "         sum(\"ad_revenue\").alias(\"ad_rev\")) \\\n",
    "    .collect()\n",
    "r2_time = time.time() - r2_start\n",
    " \n",
    "# Report 3: Subscription Analysis\n",
    "r3_start = time.time()\n",
    "report_3 = revenue.groupBy(\"plan\") \\\n",
    "    .agg(countDistinct(\"user_id\").alias(\"users\"),\n",
    "         count(\"*\").alias(\"total_streams\"),\n",
    "         avg(\"duration_sec\").alias(\"avg_duration\")) \\\n",
    "    .collect()\n",
    "r3_time = time.time() - r3_start\n",
    " \n",
    "# Report 4: Ad Performance\n",
    "r4_start = time.time()\n",
    "report_4 = revenue.groupBy(\"device\", \"genre\") \\\n",
    "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
    "         count(\"*\").alias(\"impressions\")) \\\n",
    "    .collect()\n",
    "r4_time = time.time() - r4_start\n",
    " \n",
    "# Report 5: Artist Payouts\n",
    "r5_start = time.time()\n",
    "report_5 = revenue.groupBy(\"artist_id\", \"label_type\") \\\n",
    "    .agg(sum(\"stream_payout\").alias(\"total_payout\"),\n",
    "         count(\"*\").alias(\"total_streams\")) \\\n",
    "    .orderBy(desc(\"total_payout\")).limit(100) \\\n",
    "    .collect()\n",
    "r5_time = time.time() - r5_start\n",
    " \n",
    "# Report 6: Daily Summary\n",
    "r6_start = time.time()\n",
    "report_6 = revenue.groupBy(\"event_date\") \\\n",
    "    .agg(count(\"*\").alias(\"streams\"),\n",
    "         sum(\"ad_revenue\").alias(\"ad_rev\"),\n",
    "         countDistinct(\"user_id\").alias(\"unique_users\")) \\\n",
    "    .orderBy(\"event_date\") \\\n",
    "    .collect()\n",
    "r6_time = time.time() - r6_start\n",
    " \n",
    "reduce_shuffle_total = time.time() - total_start\n",
    " \n",
    "print(f\"\\nReport 1 (genre):        {r1_time:.2f}s\")\n",
    "print(f\"Report 2 (regional):     {r2_time:.2f}s\")\n",
    "print(f\"Report 3 (subscription): {r3_time:.2f}s\")\n",
    "print(f\"Report 4 (ad perf):      {r4_time:.2f}s\")\n",
    "print(f\"Report 5 (payouts):      {r5_time:.2f}s\")\n",
    "print(f\"Report 6 (daily):        {r6_time:.2f}s\")\n",
    "print(f\"\\n⏱️  OPTIMIZED BROADCAST + CACHHED + REDUCE SHUFFLE TOTAL: {reduce_shuffle_total:.2f}s\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4035edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare reduce shuffle partition  worked \n",
    "revenue.groupBy(\"genre\")\\\n",
    "    .agg(sum(\"ad_revenue\"))\\\n",
    "        .explain(mode = \"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c090ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING BROADCAST + CACHED + REDUCE SHUFFLE PIPELINE + PRUNED \n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report 1 (genre):        0.25s\n",
      "Report 2 (regional):     0.10s\n",
      "Report 3 (subscription): 0.16s\n",
      "Report 4 (ad perf):      0.06s\n",
      "Report 5 (payouts):      0.10s\n",
      "Report 6 (daily):        0.22s\n",
      "\n",
      "⏱️  OPTIMIZED BROADCAST + CACHHED + REDUCE SHUFFLE + PRUNED TOTAL: 2.63s\n"
     ]
    }
   ],
   "source": [
    "# Optimization 4: Column pruning — select only needed columns from each table\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING BROADCAST + CACHED + REDUCE SHUFFLE PIPELINE + PRUNED \")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_start = time.time()\n",
    "# Prune colums at source\n",
    "events_pruned = events.select(\n",
    "    \"event_id\",\n",
    "    \"user_id\",\n",
    "    \"artist_id\",\n",
    "    \"genre\",\n",
    "    \"device\",\n",
    "    \"region\",\n",
    "    \"event_date\",\n",
    "    \"duration_sec\",\n",
    "    \"completed\",\n",
    "    \"month\"\n",
    ")\n",
    "\n",
    "subscriptions_pruned = subscriptions.select(\n",
    "    \"user_id\",\n",
    "    \"plan\",\n",
    "    \"country\"\n",
    ")\n",
    "\n",
    "ad_rates_pruned = ad_rates.select(\n",
    "    col(\"ad_genre\"),\n",
    "    col(\"ad_device\"),\n",
    "    col(\"cpm\")\n",
    ")\n",
    "\n",
    "payouts_pruned = payouts.select(\n",
    "    \"artist_id\",\n",
    "    \"label_type\",\n",
    "    \"per_stream_rate\"\n",
    ")\n",
    "\n",
    "# Step 2: Build optimized, cached revenue DataFrame\n",
    "def build_revenue_pruned():\n",
    "    return events_pruned \\\n",
    "        .join(broadcast(subscriptions_pruned), \"user_id\") \\\n",
    "        .join(\n",
    "            broadcast(ad_rates_pruned),\n",
    "            (events_pruned.genre == ad_rates_pruned.ad_genre) &\n",
    "            (events_pruned.device == ad_rates_pruned.ad_device)\n",
    "        ) \\\n",
    "        .join(broadcast(payouts_pruned), \"artist_id\") \\\n",
    "        .withColumn(\"ad_revenue\", col(\"cpm\") / 1000) \\\n",
    "        .withColumn(\"stream_payout\", col(\"per_stream_rate\")) \\\n",
    "        .withColumn(\"is_premium\", when(col(\"plan\") != \"free\", True).otherwise(False))\n",
    "        \n",
    "# Step 3: Cache and materialize\n",
    "revenue = build_revenue_pruned()\n",
    "revenue.cache()\n",
    "revenue.count()\n",
    "\n",
    "# Step 4:  build reports \n",
    "\n",
    "# Report 1: Genre Revenue\n",
    "r1_start = time.time()\n",
    "report_1 = revenue.groupBy(\"genre\") \\\n",
    "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
    "         countDistinct(\"user_id\").alias(\"unique_listeners\")) \\\n",
    "    .collect()\n",
    "r1_time = time.time() - r1_start\n",
    " \n",
    "# Report 2: Regional Breakdown\n",
    "r2_start = time.time()\n",
    "report_2 = revenue.groupBy(\"region\", \"country\") \\\n",
    "    .agg(count(\"*\").alias(\"streams\"),\n",
    "         sum(\"ad_revenue\").alias(\"ad_rev\")) \\\n",
    "    .collect()\n",
    "r2_time = time.time() - r2_start\n",
    " \n",
    "# Report 3: Subscription Analysis\n",
    "r3_start = time.time()\n",
    "report_3 = revenue.groupBy(\"plan\") \\\n",
    "    .agg(countDistinct(\"user_id\").alias(\"users\"),\n",
    "         count(\"*\").alias(\"total_streams\"),\n",
    "         avg(\"duration_sec\").alias(\"avg_duration\")) \\\n",
    "    .collect()\n",
    "r3_time = time.time() - r3_start\n",
    " \n",
    "# Report 4: Ad Performance\n",
    "r4_start = time.time()\n",
    "report_4 = revenue.groupBy(\"device\", \"genre\") \\\n",
    "    .agg(sum(\"ad_revenue\").alias(\"total_ad_rev\"),\n",
    "         count(\"*\").alias(\"impressions\")) \\\n",
    "    .collect()\n",
    "r4_time = time.time() - r4_start\n",
    " \n",
    "# Report 5: Artist Payouts\n",
    "r5_start = time.time()\n",
    "report_5 = revenue.groupBy(\"artist_id\", \"label_type\") \\\n",
    "    .agg(sum(\"stream_payout\").alias(\"total_payout\"),\n",
    "         count(\"*\").alias(\"total_streams\")) \\\n",
    "    .orderBy(desc(\"total_payout\")).limit(100) \\\n",
    "    .collect()\n",
    "r5_time = time.time() - r5_start\n",
    " \n",
    "# Report 6: Daily Summary\n",
    "r6_start = time.time()\n",
    "report_6 = revenue.groupBy(\"event_date\") \\\n",
    "    .agg(count(\"*\").alias(\"streams\"),\n",
    "         sum(\"ad_revenue\").alias(\"ad_rev\"),\n",
    "         countDistinct(\"user_id\").alias(\"unique_users\")) \\\n",
    "    .orderBy(\"event_date\") \\\n",
    "    .collect()\n",
    "r6_time = time.time() - r6_start\n",
    " \n",
    "pruned_total = time.time() - total_start\n",
    " \n",
    "print(f\"\\nReport 1 (genre):        {r1_time:.2f}s\")\n",
    "print(f\"Report 2 (regional):     {r2_time:.2f}s\")\n",
    "print(f\"Report 3 (subscription): {r3_time:.2f}s\")\n",
    "print(f\"Report 4 (ad perf):      {r4_time:.2f}s\")\n",
    "print(f\"Report 5 (payouts):      {r5_time:.2f}s\")\n",
    "print(f\"Report 6 (daily):        {r6_time:.2f}s\")\n",
    "print(f\"\\n⏱️  OPTIMIZED BROADCAST + CACHHED + REDUCE SHUFFLE + PRUNED TOTAL: {pruned_total:.2f}s\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare column pruning  worked \n",
    "revenue.groupBy(\"genre\")\\\n",
    "    .agg(sum(\"ad_revenue\"))\\\n",
    "        .explain(mode = \"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18709e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING FULLY OPTIMIZED PIPELINE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cached 600000 rows in 1.32s\n",
      "\n",
      "⏱️  OPTIMIZED TOTAL: 2.61s\n",
      "⏱️  BASELINE TOTAL:  45.59s\n",
      "📈 SPEEDUP:          17.5x\n",
      "📉 TIME SAVED:       42.98s (94%)\n",
      "\n",
      "OPTIMIZED PLAN:\n",
      "== Physical Plan ==\n",
      "* HashAggregate (26)\n",
      "+- Exchange (25)\n",
      "   +- * HashAggregate (24)\n",
      "      +- InMemoryTableScan (1)\n",
      "            +- InMemoryRelation (2)\n",
      "                  +- * Project (23)\n",
      "                     +- * BroadcastHashJoin Inner BuildRight (22)\n",
      "                        :- * Project (17)\n",
      "                        :  +- * BroadcastHashJoin Inner BuildRight (16)\n",
      "                        :     :- * Project (11)\n",
      "                        :     :  +- * BroadcastHashJoin Inner BuildRight (10)\n",
      "                        :     :     :- * Filter (5)\n",
      "                        :     :     :  +- * ColumnarToRow (4)\n",
      "                        :     :     :     +- Scan parquet  (3)\n",
      "                        :     :     +- BroadcastExchange (9)\n",
      "                        :     :        +- * Filter (8)\n",
      "                        :     :           +- * ColumnarToRow (7)\n",
      "                        :     :              +- Scan parquet  (6)\n",
      "                        :     +- BroadcastExchange (15)\n",
      "                        :        +- * Filter (14)\n",
      "                        :           +- * ColumnarToRow (13)\n",
      "                        :              +- Scan parquet  (12)\n",
      "                        +- BroadcastExchange (21)\n",
      "                           +- * Filter (20)\n",
      "                              +- * ColumnarToRow (19)\n",
      "                                 +- Scan parquet  (18)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [2]: [genre#15068, ad_revenue#19221]\n",
      "Arguments: [genre#15068, ad_revenue#19221]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [artist_id#15067, user_id#15066, event_id#15065, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074, plan#15086, country#15088, cpm#15095, per_stream_rate#15100, label_type#15101, ad_revenue#19221, stream_payout#19240], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@48309701,StorageLevel(disk, memory, deserialized, 1 replicas),*(4) Project [artist_id#15067, user_id#15066, event_id#15065, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074, plan#15086, country#15088, cpm#15095, per_stream_rate#15100, label_type#15101, (cpm#15095 / 1000.0) AS ad_revenue#19221, per_stream_rate#15100 AS stream_payout#19240]\n",
      "+- *(4) BroadcastHashJoin [artist_id#15067], [artist_id#15099], Inner, BuildRight, false\n",
      "   :- *(4) Project [user_id#15066, event_id#15065, artist_id#15067, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074, plan#15086, country#15088, cpm#15095]\n",
      "   :  +- *(4) BroadcastHashJoin [genre#15068, device#15072], [ad_genre#15093, ad_device#15094], Inner, BuildRight, false\n",
      "   :     :- *(4) Project [user_id#15066, event_id#15065, artist_id#15067, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074, plan#15086, country#15088]\n",
      "   :     :  +- *(4) BroadcastHashJoin [user_id#15066], [user_id#15085], Inner, BuildRight, false\n",
      "   :     :     :- *(4) Filter (((isnotnull(user_id#15066) AND isnotnull(genre#15068)) AND isnotnull(device#15072)) AND isnotnull(artist_id#15067))\n",
      "   :     :     :  +- *(4) ColumnarToRow\n",
      "   :     :     :     +- FileScan parquet [event_id#15065,user_id#15066,artist_id#15067,genre#15068,region#15069,duration_sec#15070L,completed#15071,device#15072,event_date#15073,month#15074] Batched: true, DataFilters: [isnotnull(user_id#15066), isnotnull(genre#15068), isnotnull(device#15072), isnotnull(artist_id#1..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/re..., PartitionFilters: [], PushedFilters: [IsNotNull(user_id), IsNotNull(genre), IsNotNull(device), IsNotNull(artist_id)], ReadSchema: struct<event_id:string,user_id:string,artist_id:string,genre:string,region:string,duration_sec:bi...\n",
      "   :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=6832]\n",
      "   :     :        +- *(1) Filter isnotnull(user_id#15085)\n",
      "   :     :           +- *(1) ColumnarToRow\n",
      "   :     :              +- FileScan parquet [user_id#15085,plan#15086,country#15088] Batched: true, DataFilters: [isnotnull(user_id#15085)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/re..., PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:string,plan:string,country:string>\n",
      "   :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false], input[1, string, false]),false), [plan_id=6840]\n",
      "   :        +- *(2) Filter (isnotnull(ad_genre#15093) AND isnotnull(ad_device#15094))\n",
      "   :           +- *(2) ColumnarToRow\n",
      "   :              +- FileScan parquet [ad_genre#15093,ad_device#15094,cpm#15095] Batched: true, DataFilters: [isnotnull(ad_genre#15093), isnotnull(ad_device#15094)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/re..., PartitionFilters: [], PushedFilters: [IsNotNull(ad_genre), IsNotNull(ad_device)], ReadSchema: struct<ad_genre:string,ad_device:string,cpm:double>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=6848]\n",
      "      +- *(3) Filter isnotnull(artist_id#15099)\n",
      "         +- *(3) ColumnarToRow\n",
      "            +- FileScan parquet [artist_id#15099,per_stream_rate#15100,label_type#15101] Batched: true, DataFilters: [isnotnull(artist_id#15099)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/re..., PartitionFilters: [], PushedFilters: [IsNotNull(artist_id)], ReadSchema: struct<artist_id:string,per_stream_rate:double,label_type:string>\n",
      ",None)\n",
      "\n",
      "(3) Scan parquet \n",
      "Output [10]: [event_id#15065, user_id#15066, artist_id#15067, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/events]\n",
      "PushedFilters: [IsNotNull(user_id), IsNotNull(genre), IsNotNull(device), IsNotNull(artist_id)]\n",
      "ReadSchema: struct<event_id:string,user_id:string,artist_id:string,genre:string,region:string,duration_sec:bigint,completed:boolean,device:string,event_date:date,month:int>\n",
      "\n",
      "(4) ColumnarToRow [codegen id : 4]\n",
      "Input [10]: [event_id#15065, user_id#15066, artist_id#15067, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074]\n",
      "\n",
      "(5) Filter [codegen id : 4]\n",
      "Input [10]: [event_id#15065, user_id#15066, artist_id#15067, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074]\n",
      "Condition : (((isnotnull(user_id#15066) AND isnotnull(genre#15068)) AND isnotnull(device#15072)) AND isnotnull(artist_id#15067))\n",
      "\n",
      "(6) Scan parquet \n",
      "Output [3]: [user_id#15085, plan#15086, country#15088]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/subscriptions]\n",
      "PushedFilters: [IsNotNull(user_id)]\n",
      "ReadSchema: struct<user_id:string,plan:string,country:string>\n",
      "\n",
      "(7) ColumnarToRow [codegen id : 1]\n",
      "Input [3]: [user_id#15085, plan#15086, country#15088]\n",
      "\n",
      "(8) Filter [codegen id : 1]\n",
      "Input [3]: [user_id#15085, plan#15086, country#15088]\n",
      "Condition : isnotnull(user_id#15085)\n",
      "\n",
      "(9) BroadcastExchange\n",
      "Input [3]: [user_id#15085, plan#15086, country#15088]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=7405]\n",
      "\n",
      "(10) BroadcastHashJoin [codegen id : 4]\n",
      "Left keys [1]: [user_id#15066]\n",
      "Right keys [1]: [user_id#15085]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(11) Project [codegen id : 4]\n",
      "Output [12]: [user_id#15066, event_id#15065, artist_id#15067, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074, plan#15086, country#15088]\n",
      "Input [13]: [event_id#15065, user_id#15066, artist_id#15067, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074, user_id#15085, plan#15086, country#15088]\n",
      "\n",
      "(12) Scan parquet \n",
      "Output [3]: [ad_genre#15093, ad_device#15094, cpm#15095]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/ad_rates]\n",
      "PushedFilters: [IsNotNull(ad_genre), IsNotNull(ad_device)]\n",
      "ReadSchema: struct<ad_genre:string,ad_device:string,cpm:double>\n",
      "\n",
      "(13) ColumnarToRow [codegen id : 2]\n",
      "Input [3]: [ad_genre#15093, ad_device#15094, cpm#15095]\n",
      "\n",
      "(14) Filter [codegen id : 2]\n",
      "Input [3]: [ad_genre#15093, ad_device#15094, cpm#15095]\n",
      "Condition : (isnotnull(ad_genre#15093) AND isnotnull(ad_device#15094))\n",
      "\n",
      "(15) BroadcastExchange\n",
      "Input [3]: [ad_genre#15093, ad_device#15094, cpm#15095]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false], input[1, string, false]),false), [plan_id=7414]\n",
      "\n",
      "(16) BroadcastHashJoin [codegen id : 4]\n",
      "Left keys [2]: [genre#15068, device#15072]\n",
      "Right keys [2]: [ad_genre#15093, ad_device#15094]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(17) Project [codegen id : 4]\n",
      "Output [13]: [user_id#15066, event_id#15065, artist_id#15067, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074, plan#15086, country#15088, cpm#15095]\n",
      "Input [15]: [user_id#15066, event_id#15065, artist_id#15067, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074, plan#15086, country#15088, ad_genre#15093, ad_device#15094, cpm#15095]\n",
      "\n",
      "(18) Scan parquet \n",
      "Output [3]: [artist_id#15099, per_stream_rate#15100, label_type#15101]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/atipahadiya/Documents/DE-ClassWork/week-4/day-3/M18-Lab/revenue_data/payouts]\n",
      "PushedFilters: [IsNotNull(artist_id)]\n",
      "ReadSchema: struct<artist_id:string,per_stream_rate:double,label_type:string>\n",
      "\n",
      "(19) ColumnarToRow [codegen id : 3]\n",
      "Input [3]: [artist_id#15099, per_stream_rate#15100, label_type#15101]\n",
      "\n",
      "(20) Filter [codegen id : 3]\n",
      "Input [3]: [artist_id#15099, per_stream_rate#15100, label_type#15101]\n",
      "Condition : isnotnull(artist_id#15099)\n",
      "\n",
      "(21) BroadcastExchange\n",
      "Input [3]: [artist_id#15099, per_stream_rate#15100, label_type#15101]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=7423]\n",
      "\n",
      "(22) BroadcastHashJoin [codegen id : 4]\n",
      "Left keys [1]: [artist_id#15067]\n",
      "Right keys [1]: [artist_id#15099]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(23) Project [codegen id : 4]\n",
      "Output [17]: [artist_id#15067, user_id#15066, event_id#15065, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074, plan#15086, country#15088, cpm#15095, per_stream_rate#15100, label_type#15101, (cpm#15095 / 1000.0) AS ad_revenue#19221, per_stream_rate#15100 AS stream_payout#19240]\n",
      "Input [16]: [user_id#15066, event_id#15065, artist_id#15067, genre#15068, region#15069, duration_sec#15070L, completed#15071, device#15072, event_date#15073, month#15074, plan#15086, country#15088, cpm#15095, artist_id#15099, per_stream_rate#15100, label_type#15101]\n",
      "\n",
      "(24) HashAggregate [codegen id : 1]\n",
      "Input [2]: [genre#15068, ad_revenue#19221]\n",
      "Keys [1]: [genre#15068]\n",
      "Functions [1]: [partial_sum(ad_revenue#19221)]\n",
      "Aggregate Attributes [1]: [sum#22854]\n",
      "Results [2]: [genre#15068, sum#22855]\n",
      "\n",
      "(25) Exchange\n",
      "Input [2]: [genre#15068, sum#22855]\n",
      "Arguments: hashpartitioning(genre#15068, 8), ENSURE_REQUIREMENTS, [plan_id=7385]\n",
      "\n",
      "(26) HashAggregate [codegen id : 2]\n",
      "Input [2]: [genre#15068, sum#22855]\n",
      "Keys [1]: [genre#15068]\n",
      "Functions [1]: [sum(ad_revenue#19221)]\n",
      "Aggregate Attributes [1]: [sum(ad_revenue#19221)#22595]\n",
      "Results [2]: [genre#15068, sum(ad_revenue#19221)#22595 AS sum(ad_revenue)#22596]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[artist_id: string, user_id: string, event_id: string, genre: string, region: string, duration_sec: bigint, completed: boolean, device: string, event_date: date, month: int, plan: string, country: string, cpm: double, per_stream_rate: double, label_type: string, ad_revenue: double, stream_payout: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the Fully Optimized Pipeline\n",
    "# Combine ALL optimizations into a production-ready pipeline\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING FULLY OPTIMIZED PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reset config\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "# Build enriched DataFrame ONCE with all optimizations\n",
    "revenue_opt = events \\\n",
    "    .select(\"event_id\", \"user_id\", \"artist_id\", \"genre\", \"region\",\n",
    "            \"duration_sec\", \"completed\", \"device\", \"event_date\", \"month\") \\\n",
    "    .join(subscriptions.select(\"user_id\", \"plan\", \"country\"), \"user_id\") \\\n",
    "    .join(broadcast(ad_rates),\n",
    "          (col(\"genre\") == col(\"ad_genre\")) & (col(\"device\") == col(\"ad_device\"))) \\\n",
    "    .join(broadcast(payouts), \"artist_id\") \\\n",
    "    .withColumn(\"ad_revenue\", col(\"cpm\") / 1000) \\\n",
    "    .withColumn(\"stream_payout\", col(\"per_stream_rate\")) \\\n",
    "    .drop(\"ad_genre\", \"ad_device\")\n",
    "\n",
    "# Cache the shared DataFrame\n",
    "revenue_opt.cache()\n",
    "cache_start = time.time()\n",
    "row_count = revenue_opt.count()\n",
    "cache_time = time.time() - cache_start\n",
    "print(f\"✅ Cached {row_count} rows in {cache_time:.2f}s\")\n",
    "\n",
    "# Run all 6 reports from cache\n",
    "r1 = revenue_opt.groupBy(\"genre\").agg(sum(\"ad_revenue\"), countDistinct(\"user_id\")).collect()\n",
    "r2 = revenue_opt.groupBy(\"region\", \"country\").agg(count(\"*\"), sum(\"ad_revenue\")).collect()\n",
    "r3 = revenue_opt.groupBy(\"plan\").agg(countDistinct(\"user_id\"), count(\"*\"), avg(\"duration_sec\")).collect()\n",
    "r4 = revenue_opt.groupBy(\"device\", \"genre\").agg(sum(\"ad_revenue\"), count(\"*\")).collect()\n",
    "r5 = revenue_opt.groupBy(\"artist_id\", \"label_type\") \\\n",
    "    .agg(sum(\"stream_payout\"), count(\"*\")) \\\n",
    "    .orderBy(desc(\"sum(stream_payout)\")).limit(100).collect()\n",
    "r6 = revenue_opt.groupBy(\"event_date\") \\\n",
    "    .agg(count(\"*\"), sum(\"ad_revenue\"), countDistinct(\"user_id\")) \\\n",
    "    .orderBy(\"event_date\").collect()\n",
    "\n",
    "optimized_total = time.time() - total_start\n",
    "\n",
    "print(f\"\\n⏱️  OPTIMIZED TOTAL: {optimized_total:.2f}s\")\n",
    "print(f\"⏱️  BASELINE TOTAL:  {baseline_total:.2f}s\")\n",
    "print(f\"📈 SPEEDUP:          {baseline_total/optimized_total:.1f}x\")\n",
    "print(f\"📉 TIME SAVED:       {baseline_total - optimized_total:.2f}s ({(1-optimized_total/baseline_total)*100:.0f}%)\")\n",
    "\n",
    "# Verify the plan\n",
    "print(\"\\nOPTIMIZED PLAN:\")\n",
    "revenue_opt.groupBy(\"genre\").agg(sum(\"ad_revenue\")).explain(mode=\"formatted\")\n",
    "\n",
    "revenue_opt.unpersist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09e073a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "OPTIMIZATION REPORT — StreamPulse Revenue Pipeline\n",
      "=================================================================\n",
      "\n",
      "Pipeline: Revenue Analytics (6 reports from joined data)\n",
      "\n",
      "CONFIGURATION CHANGES:\n",
      "  spark.sql.autoBroadcastJoinThreshold: -1 → 10MB\n",
      "  spark.sql.shuffle.partitions: 200 → 8\n",
      "  spark.sql.adaptive.enabled: false → (unchanged for testing)\n",
      "\n",
      "CODE CHANGES:\n",
      "  1. broadcast() on ad_rates (48 rows) and payouts (5K rows)\n",
      "  2. .cache() on enriched DataFrame (built once, used 6 times)\n",
      "  3. Column pruning on all source tables\n",
      "  4. Single build_revenue() call instead of 6 separate calls\n",
      "\n",
      "RESULTS:\n",
      "  Baseline:  45.59s\n",
      "  Optimized: 2.61s\n",
      "  Speedup:   17.5x\n",
      "\n",
      "PLAN IMPROVEMENTS:\n",
      "  - SortMergeJoin → BroadcastHashJoin (ad_rates, payouts)\n",
      "  - 6 full recomputations → 1 computation + 5 cache reads\n",
      "  - 200 shuffle partitions → 8 (matched to local cores)\n",
      "  - ReadSchema reduced (column pruning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"OPTIMIZATION REPORT — StreamPulse Revenue Pipeline\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(f\"\"\"\n",
    "Pipeline: Revenue Analytics (6 reports from joined data)\n",
    "\n",
    "CONFIGURATION CHANGES:\n",
    "  spark.sql.autoBroadcastJoinThreshold: -1 → 10MB\n",
    "  spark.sql.shuffle.partitions: 200 → 8\n",
    "  spark.sql.adaptive.enabled: false → (unchanged for testing)\n",
    "\n",
    "CODE CHANGES:\n",
    "  1. broadcast() on ad_rates (48 rows) and payouts (5K rows)\n",
    "  2. .cache() on enriched DataFrame (built once, used 6 times)\n",
    "  3. Column pruning on all source tables\n",
    "  4. Single build_revenue() call instead of 6 separate calls\n",
    "\n",
    "RESULTS:\n",
    "  Baseline:  {baseline_total:.2f}s\n",
    "  Optimized: {optimized_total:.2f}s\n",
    "  Speedup:   {baseline_total/optimized_total:.1f}x\n",
    "\n",
    "PLAN IMPROVEMENTS:\n",
    "  - SortMergeJoin → BroadcastHashJoin (ad_rates, payouts)\n",
    "  - 6 full recomputations → 1 computation + 5 cache reads\n",
    "  - 200 shuffle partitions → 8 (matched to local cores)\n",
    "  - ReadSchema reduced (column pruning)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0322df",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab18_evnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
